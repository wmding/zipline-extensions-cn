**统计学习** 利用计算机基于数据构建概率统计模型并运用模型对数据进行预测分析, 又名**统计机器学习**.

- 统计学习对象: 

  对象即为数据, 基本假设为同一类数据具有某种共同的性质, 有一定的统计规律性. 数据分连续数据和离散数据.

- 方法:

  分为**监督学习**, **无监督学习**以及**强化学习**. 学习方法三要素:模型假设, 模型选择准则, 以及模型学习的算法.

# 统计学习分类

## 基本分类

### 监督学习

- 输入空间,特征空间和输出空间

  每个具体的输入称为实例(instance), 通常用特征向量表示, 特征向量组成特征空间, 特征空间的一维代表一类特征:  
$$
x=(x^{(1)},x^{(2)},\cdots,x^{(n)} )^T
$$
  有多个输入量时, 第$i$个实例表示为:
$$
  x=(x^{(1)}_i,x^{(2)}_i,\cdots,x^{(n)}_i )^T
$$
  输出空间与输入空间类似, 一般用$Y$表示输出变量, $y$表示输出变量取值; $X$表示输入变量, $x$表示输入变量取值.

  监督学习从训练集合学习模型, 对测试集合进行预测.

  输入变量和输出变量为连续取值时, 其预测问题称为**回归问题**; 输出变量为离散变量时, 其预测问题称为**分类问题**; 输入变量和输出变量均为变量序列的预测问题时称为**标注问题**(标记问题是分类问题的一个推广, 输入是一个观测序列，输出的是一个标记序列或状态序列).

- 模型空间

  也叫假设空间, 是输入空间到输出空间映射的集合.模型表示可以是概率模型也可以是非概率模型, 可以用$P(Y|X)$或$y=f(x)$表示. 

- 训练/预测

  通过训练集合,得到一个模型, 表示为$\hat{P}(Y|X)$或者$Y=\hat{f}(x)$, 对于测试集合中的输入$x_{N+1}$, 得出预测值$y_{N+1}=\mathop{\arg\max}_{y} \hat{P}(y|x_{N+1})$或者$y_{N+1}=\hat{f}(x_{N+1})$

### 无监督学习

现实生活中常常会有这样的问题：缺乏足够的[先验知识](https://baike.baidu.com/item/先验知识/4131202)，因此难以人工标注类别或进行人工类别标注的成本太高。很自然地，我们希望计算机能代我们完成这些工作，或至少提供一些帮助。根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。

无监督学习的本质是学习数据中的统计规律或潜在结构. 实现对数据的聚类,降维或者概率估计.

### 强化学习

强化学习是智能体（Agent）以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使智能体获得最大的奖赏，强化学习不同于连接主义学习中的监督学习，主要表现在强化信号上，强化学习中由环境提供的强化信号是对产生动作的好坏作一种评价(通常为标量信号)，而不是告诉强化学习系统RLS(reinforcement learning system)如何去产生正确的动作。由于外部环境提供的信息很少，RLS必须靠自身的经历进行学习。通过这种方式，RLS在行动-评价的环境中获得知识，改进行动方案以适应环境。

### 半监督学习和主动学习

## 按模型分类

1. 概率模型和非概率模型

   - 概率模型取条件概率形式, 非概率分布取函数形式, 概率模型有

     逻辑斯谛回归既是概率模型也是非概率模型

   - 条件概率分布最大化后得到函数, 函数归一化后得到条件概率分布

   - **概率图模型**, 典型的概率模型:

     联合概率分布可以由有向图和无向图表示, 可以根据图的结构分解为因子乘积的形式. 无论模型如何复杂都可以用最基本的加法/乘法规则进行概率推理
     $$
     P(x)=\sum_yP(x,y)\\
     P(x,y)=P(x)P(y|x)
     $$
     

2. 线性模型和非线性模型

   深度学习实际上是复杂神经网络的学习 , 也就是复杂非线性模型学习

3. 参数化模型和非参模型

   模型的参数个数是否固定

## 按算法分类

在线学习和批量学习. 在线学习每接受一个样本进行预测, 之后学习模型, 不断重复.批量学习是一次接受所有数据, 学习模型然后预测.

## 按技巧分类

1. 贝叶斯推断(beyes inference): 计算模型在给定数据下的条件概率, 即后验概率.
   $$
   P(\theta|D)=\frac{P(D|\theta)P(\theta)}{P(D)}
   $$
   模型估计时, 如果需要给出一个模型, 通常取后验概率最大的模型.

   预测时, 计算数据对后验概率分布的期望值:
   $$
   P(x|D)=\int P(x|\theta, D)P(\theta|D)d\theta
   $$
   当$P(\theta)$为均匀分布时, 贝叶斯估计等价于最大似然估计.

2. 核方法

   **核方法** 是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。核方法不仅仅用于SVM，还可以用于其他数据为非线性可分的算法。核方法的理论基础是Cover's theorem，指的是对于非线性可分的训练集，可以大概率通过将其非线性映射到一个高维空间来转化成线性可分的训练集。

   关于**核方法**的描述详细可见[知乎](https://zhuanlan.zhihu.com/p/45223109).

   **高维映射**: 将高维拟合问题转变为低维问题

   **核函数**: 高维映射函数求内积 得到低维结果.不同的核函数定义了不同的**高维映射函数**.

   - 线性核: $k(x,y)=x^Ty$
   - 多项式核: $k(x,y)=(x^Ty)^d$.

# 统计学习三要素

统计学习方法一般为:
$$
方法=模型+策略+算法
$$

### 模型

模型的假设空间包含所有的可能的条件概率分布或决策函数.该假设空间通常决定于一个参数空间的参数向量.

### 策略

策略的定义是按什么准则学习或选择最优的模型.

1. 损失函数和风险函数

   **损失函数**是预测值$f(X)$与真实值$Y$的非负实值函数$L(Y,f(x))$.

   - 0-1损失函数
     $$
     L(Y,f(X))=
     \begin{cases}
     	1, & Y\neq f(X)\\
     	0, & Y=f(X)
     \end{cases}
     $$

   - 平方损失
     $$
     L(Y,f(x))=(Y-f(X))^2
     $$

   - 绝对损失

   - 对数损失

   损失函数的期望:
   $$
   R_{exp}(f)=\int_{X\times Y}{L(y,f(x))P(x,y)dxdy}
   $$
   是理论模型在联合概率分布下的平均损失, 称为风险函数或期望损失. 学习的目的是风险函数最小, 实际操作中用经验风险估计.例如对于训练集合:
   $$
   T=\{(x_1,y_1), (x_2,y_2), \cdots, (x_N,y_N)\}
   $$
   得到经验风险:
   $$
   R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}{L(y_i,f(x_i))}
   $$

2. 经验风险和结构风险最小化

   经验风险最小化:

   - 损失函数为对数损失, 经验风险最小对应最大似然估计.
   - 损失函数为平方损失, 经验风险最小对于最小二乘法.

   只用经验风险最小化, 在数据量少时会产生过拟合. 因此引入结构风险最小化, 其等价于在经验风险基础上加入正则化, 用于描述模型复杂度.

   最大后验估计(MAP): 模型为概率分布, 损失函数为对数函数, 模型复杂度由先验概率给出, 这时结构风险最小等价于MAP.

### 算法

算法是只求解最优模型的具体计算方法. 通常最优化问题没有解析解, 因此需要用数值计算方法求解.

